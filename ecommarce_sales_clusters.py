# -*- coding: utf-8 -*-
"""Ecommarce_sales_clusters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12MDr6iPnBz9wXwJ0ej97bvcficEfjw-4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
uploaded=files.upload()

df=pd.read_csv("OnlineRetail[1].csv", encoding='latin1')
df.head()

"""### Why do we see the same CustomerID and InvoiceNo repeated with different StockCode?

In this dataset, each row represents an individual line item in a retail transaction.  
That means an invoice (InvoiceNo) may contain multiple different products (StockCode).  

Example:
- A customer buys 3 different items in one order → this creates 3 rows with:
  - The same CustomerID (the buyer)
  - The same InvoiceNo (the same order)
  - Different StockCode (different products)

So seeing multiple rows like:
- CustomerID: 17850
- InvoiceNo: 573021
- StockCode: 85123A / 21094 / 22423

is completely normal and expected in real business transaction data.

### Key Meaning:
- **InvoiceNo = One actual order**
- **StockCode = Different products within the same order**
- **Quantity & UnitPrice apply per product line**
- **`df` is a line-item level dataset, not order summary level**

This structure is useful because it allows analysis such as:
- RFM segmentation (customer behavior)
- Basket size & product affinity (market basket analysis)
- Total order value = sum of product line amounts
- Product-level clustering in future

"""

print("Data shape:", df.shape)
df.info()
df.isnull().sum()

df = df.dropna(subset=["CustomerID"])
df["CustomerID"] = df["CustomerID"].astype(int)
df = df.dropna(subset=["Description"])
print("datashape:", df.shape)
print("total null:",df.isnull().sum().sum())


for column in df.select_dtypes(include='number').columns:
    df = df[(df[column] > 0)]

df = df[(df["Quantity"] > 0) & (df["UnitPrice"] > 0)]
sns.boxplot(x=df["UnitPrice"])

"""Outliers in e-commerce are real customers, usually not errors
(e.g., wholesale buyers, corporate bulk orders, VIP spenders).

Removing them would erase important buying patterns.

Our models (KMeans + Random Forest) handled outliers well:

KMeans created clear segments

Random Forest still gave strong R² and low errors

## Reference date and sales column formation
"""

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format="%d-%m-%Y %H:%M")

reference_date = df["InvoiceDate"].max() + pd.Timedelta(days=1)
print(reference_date)
df['Sales'] = df['Quantity'] * df['UnitPrice']

"""### RFM ( customer-level ) dataframe"""

reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)
rfm = df.groupby('CustomerID').agg(
    Recency = ('InvoiceDate', lambda x: (reference_date - x.max()).days),
    Frequency = ('InvoiceNo', 'nunique'),
    Monetary = ('Sales', 'sum')
).reset_index()

rfm.describe()

"""### Silhouette score check for cluster"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA


#  Scale only numeric RFM data

x = rfm[["Recency", "Frequency", "Monetary"]]
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)


# Find the best K using silhouette score
print("Silhouette Scores for Different K:")
best_k = None
best_score = -1

for k in range(2, 11):
    kmeans_test = KMeans(n_clusters=k, random_state=42, n_init=10)
    test_labels = kmeans_test.fit_predict(x_scaled)
    score = silhouette_score(x_scaled, test_labels)
    print(f"k={k}, Silhouette Score={score:.4f}")

    if score > best_score:
        best_score = score
        best_k = k
print(f"\nBest K based on silhouette score is: {best_k}")

"""###WCSS Loop (Elbow Method)"""

from sklearn.cluster import KMeans

wcss = []

for k in range(2, 11):
    kmeans_test = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans_test.fit(x_scaled)
    wcss.append(kmeans_test.inertia_)   # inertia_ = WCSS

    print(f"k={k}, WCSS={kmeans_test.inertia_:.2f}")


plt.plot(range(2, 11), wcss, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("WCSS")
plt.title("Elbow Method")
plt.show()

"""## Cluster visualization"""

#3. Train final KMeans using best k

km = KMeans(n_clusters=best_k, random_state=42, n_init=10)
labels = km.fit_predict(x_scaled)
rfm["Cluster"] = labels


# 4. PCA for Visualization

pca = PCA(n_components=2)
pca_result = pca.fit_transform(x_scaled)

rfm["PCA1"] = pca_result[:, 0]
rfm["PCA2"] = pca_result[:, 1]

# 5. Plot clusters on PCA space

plt.figure(figsize=(10,6))
sns.scatterplot(data=rfm, x="PCA1", y="PCA2", hue="Cluster", palette="viridis")
plt.title(f"Customer Segments using KMeans (k={best_k})")
plt.show()

"""### Centroids"""

real_centroids=scaler.inverse_transform(km.cluster_centers_)
pd.DataFrame(real_centroids, columns=x.columns)

"""### Customer segment clustering"""

rfm["Segment"] = rfm["Cluster"].map({
    0: "Low Value / At-Risk Customers",
    1: "High Value / VIP Customers"
})

rfm["Segment"]

"""##Visualize Cluster Centers (Heatmap)"""

import seaborn as sns
plt.figure(figsize=(5,3))
sns.heatmap(pd.DataFrame(real_centroids, columns=["Recency","Frequency","Monetary"]),
            annot=True, cmap="viridis")
plt.title("Cluster Centroid Values")
plt.show()

"""## Predictive analysis for customers for each cluster"""

# SCALE FOR CLUSTERING
scaler_kmeans = StandardScaler()
rfm_scaled = scaler_kmeans.fit_transform(rfm[["Recency", "Frequency", "Monetary"]])

# K-MEANS CLUSTERING
kmeans = KMeans(n_clusters=2, random_state=42)
rfm["Cluster"] = kmeans.fit_predict(rfm_scaled)

"""Used two scalers because the first scaler was only for PCA
visualization. It was not used for clustering.

The second scaler (scaler_kmeans) is the actual scaler used for
KMeans clustering and is also used during prediction.

Therefore, prediction must use scaler_kmeans, not the PCA scaler.

"""

# Feature engineering for prediction
# Aggregated product features from df

customer_features = df.groupby('CustomerID').agg(
    TotalQuantity=('Quantity','sum'),
    UniqueProducts=('StockCode','nunique')
).reset_index()

rfm = pd.merge(rfm, customer_features, on='CustomerID', how='left')
rfm['AvgOrderValue'] = rfm['Monetary'] / rfm['Frequency']

rfm[['CustomerID','Recency','Frequency','Monetary','TotalQuantity','UniqueProducts','AvgOrderValue','Cluster']].head()

"""Scaled only 3 features (Recency, Frequency, Monetary) for KMeans because these are the RFM features used for customer clustering.

The other engineered features (TotalQuantity, UniqueProducts, AvgOrderValue)
are used later for prediction, not for clustering. They are scaled separately
inside the regression models.

So clustering uses 3 scaled features, while prediction uses 5 scaled features.

"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[["Recency", "Frequency", "Monetary"]])

#  Make the clusters
kmeans = KMeans(n_clusters=2, random_state=42)
rfm["Cluster"] = kmeans.fit_predict(rfm_scaled)

# See how many customers in each cluster
print(rfm["Cluster"].value_counts())

"""## Model Training per Cluster"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Features used for prediction
features = ["Recency", "Frequency", "TotalQuantity", "UniqueProducts", "AvgOrderValue"]

# Dictionary to store RF model for each cluster
rf_models = {}

for cluster in rfm["Cluster"].unique():

    print("\n===============================")
    print("Training Random Forest for Cluster:", cluster)
    print("===============================\n")

    cluster_data = rfm[rfm["Cluster"] == cluster]

    x = cluster_data[features]
    y = cluster_data["Monetary"]

    # Train-test split
    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.2, random_state=42
    )

    # Scaling
    scaler = StandardScaler()
    x_train_s = scaler.fit_transform(x_train)
    x_test_s = scaler.transform(x_test)

    # Random Forest model
    rf = RandomForestRegressor(random_state=42)
    rf.fit(x_train_s, y_train)
    y_pred = rf.predict(x_test_s)

    # Evaluation metrics
    r2  = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    print("R² :", round(r2, 4))
    print("MSE:", round(mse, 2))
    print("MAE:", round(mae, 2))

"""##Random Forest Regressor
Works well with raw, messy, skewed, real-world datasets
Robust to outliers (unlike Linear Regression)
Captures complex relationships between:
Quantity,
Frequency,
Product variety,
Avg order value,

Performed best in cluster-wise evaluation:
Highest R²,
Lowest errors

### Stored the model and scaler for prediction use
"""

rf_models[cluster] = {
        "model": rf,
        "scaler": scaler
    }

"""## Prediction Functions"""

def predict_customer_value(recency, frequency, total_quantity, unique_products, average_order_value):

    # 1. Compute monetary for cluster prediction
    monetary = average_order_value * frequency

    # 2. Predict cluster
    cluster_input = pd.DataFrame(
        [[recency, frequency, monetary]],
        columns=["Recency", "Frequency", "Monetary"]
    )

    cluster_scaled = scaler_kmeans.transform(cluster_input)
    cluster_no = kmeans.predict(cluster_scaled)[0]

    print("Customer belongs to Cluster:", cluster_no)

    # 3. Prepare features for monetary prediction
    new_customer = pd.DataFrame(
        [[recency, frequency, total_quantity, unique_products, average_order_value]],
        columns=["Recency","Frequency","TotalQuantity","UniqueProducts","AvgOrderValue"]
    )

    # 4. Scale using cluster-specific scaler
    selected_scaler = rf_models[cluster_no]["scaler"]
    selected_model  = rf_models[cluster_no]["model"]

    new_scaled = selected_scaler.transform(new_customer)

    # 5. Predict monetary
    predicted_value = selected_model.predict(new_scaled)[0]
    print("Predicted Monetary Value:", round(predicted_value, 2))

"""### Unkown customer prediction"""

predict_customer_value(
    recency = 20,
    frequency = 12,
    total_quantity = 200,
    unique_products = 10,
    average_order_value = 300)

